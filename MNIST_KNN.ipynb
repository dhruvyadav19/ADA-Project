{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from __future__ import print_function\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import sklearn\n",
    " \n",
    "# handle older versions of sklearn\n",
    "if int((sklearn.__version__).split(\".\")[1]) < 18:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    " \n",
    " #otherwise we're using at lease version 0.18\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    " \n",
    " # load the MNIST digits dataset\n",
    "mnist = datasets.load_digits()\n",
    " \n",
    "# take the MNIST data and construct the training and testing split, using 75% of the\n",
    "# data for training and 25% for testing\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(np.array(mnist.data),\n",
    "    mnist.target, test_size=0.25, random_state=42)\n",
    " \n",
    "# now, let's take 10% of the training data and use that for validation\n",
    "(trainData, valData, trainLabels, valLabels) = train_test_split(trainData, trainLabels,\n",
    "    test_size=0.1, random_state=84)\n",
    " \n",
    "# show the sizes of each data split\n",
    "#print(\"training data points: {}\".format(len(trainLabels)))\n",
    "#print(\"validation data points: {}\".format(len(valLabels)))\n",
    "#print(\"testing data points: {}\".format(len(testLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "achieved highest accuracy of 68.8%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize the values of k for our k-Nearest Neighbor classifier along with the\n",
    "# list of accuracies for each value of k\n",
    "kVals = range(1, 30, 2)\n",
    "accuracies = []\n",
    " \n",
    "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
    "for k in range(1, 30, 2):\n",
    "    # train the k-Nearest Neighbor classifier with the current value of `k`\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(trainData, trainLabels)\n",
    "\n",
    "    # evaluate the model and update the accuracies list\n",
    "    score = model.score(valData, valLabels)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "    accuracies.append(score)\n",
    " \n",
    " # find the value of k that has the largest accuracy\n",
    "i = int(np.argmax(accuracies))\n",
    "print(\"k=%d achieved highest accuracy of %.2f%%\" % (kVals[i],\n",
    "    accuracies[i] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
